---
title: "Statistical methods in public policy research"
subtitle: "Chapter for the *Oxford Research Encyclopedia on Public Policy*"
date: 2025-09-23
author:
- name: Andrew Heiss
  affiliations: 
    - id: gsu
      name: Georgia State University
      department: Andrew Young School of Policy Studies, Department of Public Management and Policy
      address: "55 Park Place NE, #464"
      city: Atlanta
      region: GA
      country: USA
      postal-code: 30303
  orcid: 0000-0002-3948-3914
  url: https://www.andrewheiss.com
  email: aheiss@gsu.edu

code-repo: "Access the code and data at <https://github.com/andrewheiss/snoopy-spring>"

bibliography: bib/references.json
csl: bib/apa.csl

abstract-title: "Article summary"
abstract: |
  This essay provides an overview of statistical methods in public policy, focused primarily on the United States. The essay traces the historical development of quantitative approaches in policy research, from early ad hoc applications through the 19th and early 20th centuries, to the full institutionalization of statistical analysis in federal, state, local, and nonprofit agencies by the late 20th century. The essay then outlines three core methodological approaches to policy-centered statistical research across social science disciplines: description, explanation, and prediction. In descriptive work, researchers explore *what exists* and examine any variable of interest to understand their different distributions and relationships. In explanatory work, researchers ask *why does it exist* and *how can it be influenced*. The focus of the analysis is on explanatory variables (X) to either (1) accurately estimate their relationship with an outcome variable (Y), or (2) causally attribute the effect of specific explanatory variables on outcomes. In predictive work, researchers ask *what will happen next* and focus on the outcome variable (Y) and on generating accurate forecasts, classifications, and predictions from new data. For each approach, the essay examines key techniques, their applications in policy contexts, and important methodological considerations. The discussion then considers critical perspectives on quantitative policy analysis framed around issues related to a three-part "data imperative" where governments are driven to count, gather, and learn from data. Each of these imperatives entail substantial issues related to privacy, accountability, democratic participation, and epistemic inequalities—issues at odds with public sector values of transparency and openness. The conclusion identifies some emerging trends in public sector-focused data science, inclusive ethical guidelines, open research practices, and future directions for the field.

keywords:
  - public policy
  - quantitative analysis
  - evidence-based policy
  - description
  - explanation
  - estimation
  - causal inference
  - prediction
  - econometrics

params:
  wordcount: |
    <strong>{{< words-sum body-ref >}} total words</strong>: {{< words-body >}} in the body • {{< words-ref >}} in the references
---

::: {.content-visible when-meta="wordcount-banner"}
{{< include _extensions/andrewheiss/wordcount/banner.html >}}
:::

```{r}
#| label: setup
#| include: false

if (is.null(knitr::pandoc_to())) {
  fmt_out <- "interactive"
} else {
  fmt_out <- knitr::pandoc_to()
}

knitr::opts_chunk$set(
  echo = FALSE, include = FALSE, warning = FALSE, message = FALSE,
  fig.width = 6, fig.height = (6 * 0.618), fig.retina = 3,
  out.width = "100%", fig.align = "center"
)
```

```{r}
#| label: libraries-data
library(tidyverse)
library(tinytable)

minipagify <- function(x) {
  paste0(
    "\\minipage[t]{\\textwidth}\\setlist[itemize]{nosep, leftmargin=*}", 
    trimws(markdown::mark_latex(text = x, template = FALSE)), 
    "\\vspace{2pt}\\endminipage"
  )
}

tt_linebreak <- function(text) {
  if (fmt_out == "latex") {
    map_chr(text, \(x) {
      if (is.na(x)) {
        return(x)
      }

      x |>
        str_replace_all("BBBB", "\n") |>
        str_replace_all("PPPP", "\n\n")
    })
  } else {
    map_chr(text, \(x) {
      if (is.na(x)) {
        return(x)
      }
      x |> 
        str_replace_all("BBBB", "\n") |> 
        str_replace_all("PPPP", "<br>")
    })
  }
}
```

::: {.content-visible when-format="html"}
# Introduction {.hide .unnumbered}
:::

The world is awash in data. The World Bank and the Organization for Economic Cooperation and Development (OECD) offer hundreds of country-level social and economic indicators, and thousands of other measures are available from the US Census, US state, and local government agencies. Surveys like the American Community Survey, Current Population Survey, and General Social Survey provide detailed information about states, counties, census blocks, households, and individuals. Organizations like the Jameel Poverty Action Lab (J-PAL), the Center for Effective Global Action (CEGA), the US Department of Education's What Works Clearinghouse, and the Campbell Collaboration publish data and reports from thousands of quantitative evaluations of policy interventions aimed at reducing poverty, improving health, alleviating the effects of climate change, and identifying effective education practices.

Knowing how to work with and analyze this data is a core component of public policy and administration. The Network of Schools of Public Policy, Affairs, and Administration (NASPAA) includes data analysis as one of its core competencies, and requires that students learn "to analyze, synthesize, think critically, solve problems and make evidence-informed decisions in a complex and dynamic environment" using both qualitative and quantitative data [@NASPAA:2025]. This emphasis reflects a shift in how policy decisions are made and evaluated, moving from intuition and experience to a reliance on evidence-based empirical analysis.

Quantitative public policy research is inherently interdisciplinary. As Kraft & Furlong explain, "because public problems can be understood only through the insights of many disciplines, policy analysis draws from the ideas and methods of economics, political science, sociology, psychology, philosophy, and other scientific and technical fields" [-@KraftFurlong:2015, 115]. This research serves both practical and scholarly purposes—informing actual policy decisions through applied analysis and advancing a more academic understanding of policy processes. Because of this mix of disciplinary traditions, statistical practices, goals, and terminology in policy research can vary wildly. A unified framework for understanding statistical approaches in policy research that transcends disciplinary boundaries is essential for advancing evidence-based policymaking and understanding how policy is made.

This essay provides an overview of statistical methods in public policy, focused primarily on the United States. It begins by tracing the historical development of quantitative approaches in policy research, from early ad hoc applications to the full institutionalization of statistical analysis in federal, state, local, and nonprofit agencies. The analysis then outlines three core methodological approaches to policy-centered statistical research across social science disciplines: description, explanation, and prediction. For each approach, the essay examines key techniques, their applications in policy contexts, and important methodological considerations. The discussion then considers critical perspectives on quantitative policy analysis, particularly concerns about bias, transparency, and the potential reinforcement of existing inequalities through data collection and analysis practices. The conclusion identifies emerging trends and future directions for the field.


# Brief history of statistics in public policy

Data has been used to inform government policies for centuries. Governments work to make their societies "legible" and understandable in order to better maintain control over their territories and respond to citizen demands. Technologies like cuneiform tablets documenting economic activity in ancient Sumer, Babylonian and Roman censuses, medieval English customs houses, European military records, and British cadastral maps all served as tools for governments to "see like a state" [@Scott:1998]. Data collection is even built into the United States Constitution, which mandates a decennial census for congressional apportionment (Article I, Section 2).

In the 1800s, publicly-available government data became more accessible to researchers, journalists, and policymakers, who began to use this data to lobby for legislative and social changes. For instance, in 1848, *New York Herald* editor Horace Greeley used data from the US Postal Service to show that congressional representatives were purposely overcharging travel reimbursements, leading to 1849 legislation prohibiting excess mileage charges [@Klein:2015]. During the Crimean War in the 1850s, Florence Nightengale used summary statistics and plots to advocate for improved sanitation and public health practices [@Brasseur:2005]. Later in the century, scholars and activists used data to demonstrate evidence of government-led discrimination against Black minorities throughout the United States. Ida B. Wells collected and analyzed newspaper reports and conducted extensive fieldwork to challenge the idea that Black lynching victims deserved the violence perpetrated against them due to poor morals, and instead provided evidence that lynchings were used to protect white social, economic, and political interests [@Francis:2014]. Throughout his career, W. E. B. Du Bois analyzed government-collected economic data to visualize the large racial wealth gap in post-slavery America, offering dozens of visualizations, reports, and academic papers that informed policy debates in the late 19th and early 20th centuries [@DuBoisBattle-BaptisteRusert:2018].

Most of this public sector statistical work, however, was done on an ad hoc basis. Some proposed policies were backed by quantitative evidence, but decisions were largely made through experience and craft knowledge [@FlemingRhodes:2018]. Near the end of the 20th century, Woodrow @Wilson:1887 called for the scientific study of government management and administration, arguing that systematic analysis would improve government efficiency. Though not explicitly statistical in nature, Wilson's work laid the groundwork for more quantitative approaches to governance. For instance, in the early 1900s, Charles Merriam pushed for increased statistical analysis in political science research, leading to the creation of research centers focused on quantitative political and policy studies at the University of Chicago in the 1920s, and the institutionalization of quantitative social science more broadly [@Sylvan:1991]. Merriam encouraged statistical analysis at a federal level and helped establish the Social Science Research Council, which remains a central hub for quantitative research today.

Following the Great Depression and its New Deal policy interventions, and especially after World War II, the US federal bureaucracy expanded rapidly in scope. In response, scholars called for more systematic study of the policy process, with Harold Lasswell and Daniel Lerner advocating for the creation of an interdisciplinary field of "policy sciences" in the 1950s [@LernerLasswell:1951]. Lasswell's vision was for "a more muscular and integrated version of Wilson's appeal for the scientific management of government" with research that merged Merriam-style quantitative methods with "insights from sociology, economics, business, [and] law," as well as methods from physics and biology [@Allison:2006, 63; @Lasswell:1951]. Beginning in the 1960s with the Kennedy administration, "the impulse to clarify policy options through quantification" [@Allison:2006, 64] rapidly snowballed as federal agencies borrowed and adapted statistical, game theoretical, and cost-benefit analytical approaches from the Department of Defense and the RAND Corporation. Philanthropic organizations like the Ford Foundation supported this transformation, providing millions of dollars in grants for graduate training in quantitative policy analysis throughout the 1970s [@Allison:2006]. 

This push for systematic quantitative analysis at a federal level culminated in the establishment of the Congressional Budget Office (CBO) in 1974. Under the direction of Alice Rivlin, the CBO developed rigorous statistical methods for budget forecasting and policy impact analysis, establishing standards for non-partisan statistical analysis that have been used to estimate the costs and score the impact of all proposed federal legislation. Similar legislative and cabinet-level executive agencies were founded or reorganized after the creation of the CBO, including the Congressional Research Service, the Government Accountability Office, and a host of offices with names including "policy," "planning," "evaluation," and "administration"—each with the goal of applying quantitative methods to systematically analyze the effects of proposed or existing public policies [@WeimerVining:2017, 35]. State and local governments followed suit, establishing their own policy analysis units and mandating budget forecasts and legislative scorecards. By the 1980s, policy analysis had emerged as a distinct profession.

Before the push for the rationalization of policy research in the 1970s, most public policy research relied on qualitative methods [@BreunigAhlquist:2014]. Today, however, policy analysis in the United States is largely quantitative. The experience of the *Journal of Policy Analysis and Management* (JPAM)—the current flagship journal for policy analysis—is illustrative. Founded in 1981, JPAM initially published qualitative policy work, including case studies, comparative and historical analysis, descriptive work, and theory building—from 1981–1984, only 27% of JPAM's research articles used any sort of quantitative methods.^[Figures based on author's collected data.] This ratio reversed in 2000, when only a quarter of JPAM's articles were explicitly qualitative, and between 2001 and 2016, 90% of articles published in JPAM used quantitative methods, including panel regression, experiments, econometric causal inference methods, simulations, and predictive modeling. JPAM's heavy statistical emphasis continues today. 

Some of this turn towards quantitative work in academic publishing is a function of editorial preferences, but much of it reflects increased demand for quantitative policy analysis by policy designers and policymakers who must adhere to legal requirements for rigorous evaluations. Beginning in the late 1980s, econometricians partnered with policymakers to develop research designs to test the causal effect of policy interventions and produce measurable evidence of policy impact. Agencies and researchers partnered to run large-scale randomized control trials (RCTs), like a Department of Labor-funded job training program [@LaLonde:1986], a Department of Housing and Urban Development housing voucher program named Moving to Opportunity [@NBER:2025], and a state-funded experiment on elementary school classroom sizes named Tennessee STAR [@Mosteller:1995]. The results of these RCTs coincided with developments in non-experimental causal inference work [@CardKrueger:1994; @ImbensWooldridge:2009] and helped create the "credibility revolution"—where policy evaluation is expected to have a plausible causal identification strategy to demonstrate evidence of social impact—in economics and social science more broadly [@AngristPischke:2010]. By the early 2000s, new institutions emerged to encourage, fund, support, and catalog the growing number of quantitative and causally-focused policy and program evaluation studies, including the Campbell Collaboration, which provides a central database and accompanying meta-analyses of a wide range of policy interventions; the Department of Education's What Works Clearinghouse, which houses evidence-based studies on education interventions; and the Jameel Poverty Action Lab (J-PAL), which funds, administers, and analyzes international development and poverty interventions around the world.

The growth of evidence-based quantitative policy analysis culminated in formal codification through the Foundations for Evidence-Based Policymaking Act of 2018, which mandated that federal agencies develop evidence-building plans and systematically evaluate their programs. This legislation represents the full institutionalization of statistical methods in policy analysis, reflecting both the maturation of quantitative methodologies and the belief that "government decisions should be based on rigorous evidence and data about what works" [@EvidenceAct] and that systematic analysis can improve governance outcomes.


# Core methodological approaches

Quantitative policy analysis and evaluation merges the statistical methods of multiple fields, including political science, psychology, and economics. Across these disciplines, quantitative researchers focus on both (1) characterizing individual social phenomena and their distributions and (2) analyzing relationships between phenomena. However, this interdisciplinary blend often creates terminological confusion, as different fields use distinct vocabulary for similar estimands, variables, tests, and procedures. While *terminology* might differ, methodologists have converged on similar categorizations of the *objectives* of quantitative research. Synthesizing work by @BreunigAhlquist:2014 and @Efron:2020, quantitative public policy analysis can be divided into three fundamental purposes:

1. **Description**, where the focus of the analysis is on exploring and understanding key variables, their distributions, and their relationships [@BreunigAhlquist:2014; @Tukey:1977; @Cleveland:1993; @Tufte:2001]
2. **Explanation**, where the focus of the analysis is on explanatory variables ($X$) to either (1) accurately estimate their relationship with an outcome variable ($Y$), or (2) causally attribute the effect of specific explanatory variables on outcomes [@Shmueli:2010; @Breiman:2001; @Efron:2020; @MorganWinship:2014; @AngristPischke:2008; @PearlMackenzie:2020]
3. **Prediction**, where the focus of the analysis is on the outcome variable ($Y$) and generating accurate forecasts, classifications, and predictions from new data [@BreunigAhlquist:2014; @Efron:2020]

These three categories provide a useful shorthand for describing different purposes of analysis, but they are rarely mutually exclusive. Descriptive exploratory work is necessary for both explanatory and predictive analysis, and the division between causal and non-causal inference is rarely clear-cut [@EsterlingBradySchwitzgebel:2025]. Researchers can shift between objectives during different phases of a single study, or may pursue multiple objectives simultaneously. @tbl-methods-summary summarizes these three objectives and provides some interdisciplinary disambiguation for these concepts.

```{r}
#| label: tbl-methods-summary
#| tbl-cap: Summary of objectives of statistical analysis
#| include: true

read_tsv("data/methods-table.tsv") |>
  rename(` ` = "...1") |>
  mutate(across(
    everything(),
    \(x) tt_linebreak(x)
  )) |>
  tt(width = c(0.17, 0.83 / 3, 0.83 / 3, 0.83 / 3)) |>
  (\(x) {
    if (fmt_out == "latex") {
      x |>
        format_tt(i = 3, j = 3, fn = minipagify) |>
        format_tt(i = 3, j = 4, fn = minipagify) |>
        format_tt(j = 1:2, markdown = TRUE) |>
        format_tt(i = c(1:2, 4:7), j = 3:4, markdown = TRUE)
    } else {
      x |>
        format_tt(j = 2:4, markdown = TRUE)
    }
  })() |>
  style_tt(align = "l") |>
  style_tt(j = 1, bold = TRUE) |>
  theme_html(class = "table table-sm") |>
  theme_latex(
    inner = paste0(
      "rowsep=2pt,",
      "cells={font=\\footnotesize},",
      "row{1}={valign=b, font=\\footnotesize\\bfseries},",
      "column{1}={valign=t, font=\\footnotesize\\bfseries},",
      "hline{3-Y}={0.25pt}"
    )
  )
```

## Description

With legal mandates to quantitatively measure the impact of policies and programs, a substantial amount of space in public policy statistics courses and textbooks is dedicated to hypothesis testing and other inferential techniques [@NowlinWehde:2024; @Weber:2024; @Berman:2007; @BuenodeMesquitaFowler:2021]. While these approaches are important, an overemphasis on inference or prediction before understanding the data can lead to incorrect conclusions. A key component of statistical research that should be carried out before any confirmatory or inferential data analysis is *exploratory data analysis* (EDA).

EDA represents a more formalized approach to the ad hoc visualization work common in the late 1800s and early 1900s. In the 1950s, Mary Eleanor Spear, a visual information specialist at the US Bureau of Labor Statistics, encouraged analysts to plot and explore their data, arguing that visual exploration provided "better comprehension of data than is possible with textual matter alone" [@Spear:1952, 3]. Spear's emphasis on visual exploration was later formalized by @Tukey:1965[@Tukey:1977], who argued that EDA is an iterative process where researchers examine their data to discover patterns, identify anomalies, check assumptions, and develop hypotheses. Exploratory techniques help researchers understand the structure of their data before imposing theoretical models [@Behrens:1997]. As @WickhamCetinkaya-RundelGrolemund:2023 [sec. 10.1] note, even if "primary research questions are handed to you on a platter," exploring the data remains essential for understanding its quality and discovering unexpected patterns before conducting any formal statistical tests. Descriptive EDA includes looking at raw data values; computing univariate summary statistics like means, medians, variances, standard deviations, and ranges; computing multivariate summary statistics like correlations and crosstabs; and creating data visualizations like histograms, density plots, scatterplots, bar charts, and maps [@HealyMoody:2014].

In general, descriptive work examines distributions, patterns, and relationships in data without necessarily making causal claims. It can look at variables by themselves (i.e. just $X$ or just $Y$) or at variables in the context of other variables (i.e. the general relationship between $X$ and $Y$), and it can be done as part of either inferential and predictive analysis, or as an end in itself [@Alexander:2023, chapter 11]. Basic descriptive statistics are incredibly common—and valuable—in policy research [@Berman:2007, 96]. Policymakers and managers are interested in knowing accurate estimates of all sorts of basic values, like a country's average GDP, the median unemployment rate per state, the range of PM2.5 air quality levels over the course of a year in a county, or the variance in property values within a city. For instance, @ChettyHendrenKline:2014 and @ChettyHendrenKline:2014a use detailed administrative data on more than 40 million individuals to describe general patterns of intergenerational mobility. Instead of arguing for a causal identification strategy or employing complex predictive methods, they largely rely on basic regression models, plots, and maps to illustrate different trends in mobility and inequality throughout the United States. Many large-scale descriptive projects use public data from the US Census, state records, and other sources to provide a descriptive overview of policy trends, like the @AmericanCommunitiesProject:2025, which classifies and maps US counties into a range of different social and economic communities, centers, and enclaves, or the Distressed Communities Index, which maps dozens of different economic indicators across ZIP codes [@EconomicInnovationGroup:2025]. 

These descriptive approaches can form the basis for monitoring and process evaluation work [@RossiLipseyHenry:2019] and can inform policy debates and decision-making without needing more complex explanatory or predictive approaches. For example, the Congressional Budget Office provides descriptive distributional analyses of the allocation of federal resources across various population crosstabs, like employment rates across race and income levels across family sizes [@CongressionalBudgetOffice:2025]. Similarly, public health agencies provide descriptive data on trends in disease prevalence over time and geography, which empowered policymakers and the general public during the COVID-19 pandemic [@LiYarime:2021].

Careful exploratory and descriptive analysis serves as both a foundation for more complex statistical methods and a valuable standalone tool in policy research. By revealing patterns and relationships that might otherwise remain hidden—and by providing researchers with a better understanding of their data—descriptive statistics are important for evidence-informed analysis.


## Explanation

Summary statistics can describe variables and social phenomena, but single point estimates (e.g., the average tax revenues received by a city, the average annual unemployment rate in a state, etc.) do not generally provide enough information for making decisions or conclusions about policies [@Imbens:2021]. Individual values do not indicate confidence that described characteristics reflect reality, nor do they describe how much uncertainty is inherent in those estimates [@AronowMiller:2019, p. 124]. A core element of statistical research, therefore, is *inference*, which allows researchers both to (1) quantify the uncertainty of estimated values and (2) test hypotheses about estimates' approximations of real world phenomena. Inferential approaches move beyond description and *explain* relationships between variables, providing insights into causal mechanisms that drive policy-relevant outcomes. Explanatory analysis primarily focuses on $X$ variables—also known as explanatory or independent variables—and their relationship with an outcome or dependent variable ($Y$). Explanatory analysis moves from characterizing what exists to understanding why it exists and how it might be influenced.

### Estimation, inference, and hypothesis testing

Explanatory analysis comprises two complementary processes: estimation and hypothesis testing. Estimation involves determining the magnitude of relationships between variables, or how much one or more $X$ variable is associated with or influences $Y$. The process of estimation consists of three components: an estimand, an estimator, and an estimate. Analysts first define an *estimand*, or a target quantity of interest that is based on an underlying theory to be tested [@LundbergJohnsonStewart:2021]. They then apply an *estimator*—a procedure, algorithm, or technique like subtracting two averages or fitting a regression model—to calculate an *estimate* of the target quantity [@LittleLewis:2021]. For example, a researcher might be interested in the relationship of county characteristics ($X$) on unemployment rates ($Y$). Their estimand (or target quantity) might be the difference in average unemployment rates between urban and rural counties. They would then calculate the difference in means as the estimator, resulting in an estimated difference.

Hypothesis testing evaluates whether observed estimates are statistically distinguishable from chance occurrences. Classical null hypothesis significance testing (NHST) involves comparing an observed estimate to what would be expected if there were no meaningful underlying relationship in the population. Importantly, this idea of "no relationship" does not mean a value of precisely zero; rather, it represents a distribution of values that would be considered negligible or unimportant for practical purposes, determined by the variability and sample size of the data. Hypothesis testing typically produces two key outputs: confidence intervals and *p*-values. A confidence interval provides a range of plausible values for the true parameter, with wider intervals indicating greater uncertainty. The *p*-value represents the probability of observing an estimate at least as extreme as the one calculated, if the null hypothesis is true. Analysts then must decide if there is sufficient evidence that the estimated value does not fit within the null distribution. Conventionally, 0.05 is used as an evidentiary threshold—if there is a less than 5% chance that the observed estimate could fit the null hypothesis, the estimate is considered "statistically significant" and not zero.

Standard statistical techniques such as *t*-tests, proportion tests, chi-squared tests, and linear regression are used for both calculating estimates and for providing details to test hypotheses about those estimates. For instance, a *t*-test not only estimates the difference between two group means but also tests whether that difference is statistically significant. Similarly, regression coefficients provide estimates of relationships between variables—either as slopes or shifts in intercepts—while their associated test statistics allow researchers to evaluate the statistical significance of these relationships. 

Linear regression is particularly ubiquitous in explanation-focused policy analysis, as it allows researchers to explore how multiple $X$ variables simultaneously explain a single outcome. These models are often used to examine the determinants of outcomes, like the purchase of private health insurance across a range of socioeconomic characteristics [@Gutierrez:2018], the distribution of foreign aid based on a variety of donor- and recipient-country characteristics [@Bermeo:2017], the uptake in energy efficiency tax credits across individual income levels [@Jacobsen:2019], or the adoption of nonprofit accountability practices across different organizational features [@SaxtonKuoHo:2012]. Other statistical techniques like random causal forests [@WagerAthey:2018; @AtheyTibshiraniWager:2019] can measure the relative importance of multiple $X$ variables, providing analysts with information about the salience of possible policy levers. For instance, @AksoyCarpenterDeHaas:2023 explore the effect of different experimental treatments on anti-LGBT attitudes. They report regression coefficients to demonstrate the effect of individual $X$ variables in isolation and use a random forest model to report which explanatory variables have the greatest relative influence on the outcome.

Modern statistical software makes it trivial to control for multiple explanatory variables, and it can be tempting to include as many independent variables as possible. However, this is generally a poor approach to explanatory analysis, often called "garbage can" modeling [@Achen:2005]. When explaining variation in a policy outcome, researchers should ensure that the explanatory variables they include are rooted in underlying theory. Moreover, adding extra control variables can lead to unexpected mathematical outcomes due to multicollinearity, confounding, and collider bias, and researchers must take care to not include "bad controls" in their models [@CinelliForneyPearl:2024].

### Causal attribution and causal inference

General explanatory analysis allows researchers to estimate the relationships and associations between $X$ and $Y$, but on their own, these techniques cannot speak to whether relationships are mechanistic or causal. If unemployment rates are significantly higher in rural counties than in urban counties, it does not imply that forced urbanization would be a useful policy intervention to improve employment. The old adage that "correlation is not causation" holds.

However, a special form of explanatory analysis can be used to attribute changes in an outcome to a specific program or policy, allowing analysts and policymakers to discuss the causal effects of interventions. In this approach, causal attribution can be defined using a metaphor of listening and responding: "$X$ is a cause of $Y$ if $Y$ listens to $X$ and decides its value in response to what it hears" [@PearlGlymourJewell:2016, 5–6]. This definition aligns well with the idea of interventions as levers—policymakers can develop a program or policy to improve a social outcome, and analysts can attribute how much that policy influences the variation of that outcome, providing evidence that the intervention *causes* measurable social changes.

Calculating causal estimands requires more than statistical tests—it requires an understanding of the counterfactual, or what would have happened in the absence of a policy. The potential outcomes framework formalizes this type of counterfactual thinking [@Rubin:2005]. For each unit $i$ (an individual, a county, a state, a country, etc.), two potential outcomes exist: $Y_i^1$, or the outcome if unit $i$ receives the intervention, policy, or treatment, and $Y_i^0$, or the outcome if unit $i$ does not receive the intervention. The individual causal effect for unit $i$ is the difference between these two potential outcomes, or $Y_i^1 - Y_i^0$. However, it is not possible to simultaneously observe what would happen in a state that implemented a given policy *and* what would happen in that same state at the same point in time if it did not implement that same policy. This creates the fundamental problem of causal inference: for any unit, only $Y_i^1$ or $Y_i^0$ can be observed, never both. 

Since unit-level causal effects are unobservable, researchers must estimate average treatment effects. These population-level estimands allow us to quantify the impact of policies and interventions despite our inability to observe individual counterfactuals. The estimation approach relies on comparing outcomes across different units, assuming that individual variations balance out in aggregate when proper identification strategies are employed. Two estimands are common and important in policy-related research [@GreiferStuart:2023]. The average treatment effect (ATE) is the difference between the average outcomes for treated and untreated units ($E[Y^1_i - Y^0_i]$), and represents the expected effect of a policy or intervention across the entire population. The average treatment on the treated effect (ATT) is the conditional average difference among only treated units ($E[Y^1_i - Y^0_i \mid X_i = 1]$), and represents the expected effect for those that received the intervention. More simply, the ATE represents the average effect of a policy for everyone, like the effect of a job training program on the unemployment rate for the entire state population, while the ATT represents the average effect of a policy for those who use it, like the effect of a job training program on the unemployment rate among those who participate in it.

With observational data, though, it is not possible to simply find the difference between the average outcomes for treated and untreated units. Units self-select into policies—states pass their own laws, cities develop their own programs, and individuals sign up for interventions they feel they would benefit from. As a result, observed differences between average treated and untreated outcomes suffer from selection bias, where the choice to participate in policy and the outcome are determined by confounding factors, or common causes [@PearlMackenzie:2020; @PearlGlymourJewell:2016; @Huntington-Klein:2022]. To address confounding and reduce selection bias, researchers interested in causal attribution employ various identification strategies, or sets of assumptions and techniques to isolate an unbiased estimate of the effect of an intervention on an outcome. These approaches fall into two broad categories: adjustment-based identification and circumstantial identification.

**Adjustment-based identification** In adjustment-based identification, analysts address confounding through statistical adjustment, controlling for all variables that might influence both treatment assignment and the outcome. The identification of confounding variables is typically carried out by creating structural causal models (SCMs) and drawing directed acyclic graphs (DAGs) that represent the underlying data generating process for the treatment and the outcome [@Rohrer:2018]. Following the rules of *do*-calculus, analysts can use DAGs to identify sets of covariates that need to be adjusted for to eliminate confounding [@PearlGlymourJewell:2016], as well as identify "bad controls" that should not be adjusted for [@CinelliForneyPearl:2024]—variables that, when controlled for, can actually introduce bias rather than reduce it, such as mediators (variables that lie on the causal path between treatment and outcome) and colliders (variables that are jointly caused *by* the treatment and outcome; see @KnoxLoweMummolo:2020). The actual statistical adjustment can be performed in a variety of ways, such as including all required confounders as covariates in a regression model, using confounders to calculate the propensity of treatment status and then matching observations with similar probabilities, and weighting observations by the inverse of their treatment probability (or inverse probability weighting) to create balanced groups of treated and untreated units [@HernanRobins:2024; @Heiss:2021]. 

Adjustment-based methods like propensity score matching were more common in political science and economics research in the 1990s and early 2000s [@DehejiaWahba:1999; @SmithTodd:2001], and are still occasionally used [@HeinrichMueserTroske:2013], but most empirical causal work in these disciplines now relies on circumstantial identification [see @KingNielsen:2019 for critiques of matching methods, for instance]. Methods using causal graphs and inverse probability weighting remain common in epidemiological and public health research [@HernanRobins:2024]. However, recent work has called for increased use of adjustment-based approaches in empirical political science and econometrics [@BlackwellGlynn:2018; @HuffmanVanGameren:2018], and newer econometrics-focused causal inference textbooks are structured around causal graphs [@Huntington-Klein:2022; @Huntington-Klein:2021; @Cunningham:2021]. While these adjustment-based methods rely on the difficult-to-test assumption that all confounders are observable and can be statistically adjusted, sensitivity analysis techniques allow researchers to test the robustness of estimates to unmeasured confounding [@McGowan:2022; @CinelliHazlett:2020] and enhance the plausibility of their causal identification.

**Circumstantial identification** Rather than attempt to account for all observable confounding, circumstantial identification approaches allow researchers to leverage research designs and special circumstances that create plausible exogenous variation in treatment assignments. These special circumstances can either be imposed by the researchers themselves through experimental manipulation, or can be found "in the wild" as natural or quasi-experiments.

In randomized controlled trials (RCTs), researchers randomly assign units to be treated or untreated by a policy intervention and then calculate the difference in outcomes between the two groups. The randomization process eliminates any possible confounding, since the only process that influences a unit's access to the policy is the random assignment itself, not the unit's preferences or propensity to self-select. As a result, the unbiased, unconfounded average causal effect can be calculated as a basic difference in means ($E[Y \mid X = 1] - E[Y \mid X = 0]$). As discussed earlier, RCTs have been an incredibly powerful tool for producing policy evidence, and federal and state agencies regularly fund policy and program experiments—J-PAL alone hosts a database of more than 1,200 RCTs conducted in nearly 100 countries, with evidence related to education, public health, and poverty alleviation interventions.^[See <https://www.povertyactionlab.org/evaluations>.] RCTs can also be conducted on a much smaller scale through carefully designed survey experiments, where participants are randomly assigned different tasks like reading vignettes [@Tremblay-BoirePrakash:2019; @ChaudhryHeiss:2021] or evaluating the importance of specific characteristics of hypothetical bureaucrats [@SchusterMeyer-SahlingMikkelsen:2020; @OliverosSchuster:2018] or nonprofit organizations [@ChaudhryDotsonHeiss:2025]. 

Experiments and RCTs are commonly seen as the "gold standard" in social scientific causal inference because of their plausible absence of confounding. However, treating RCTs as an unqualified gold standard overlooks several important limitations. The tension between experimental control and external validity presents a difficult challenge: tightly controlled experiments may not generalize to real-world policy contexts where compliance is imperfect and administrative capacity varies [@LuColeHowe:2022]. Many published RCTs are also statistically underpowered, lacking sufficient sample sizes to reliably detect effect sizes [@Arel-BundockBriggsDoucouliagos:2025]. However, newer methodological tools now allow researchers to declare and preregister their experimental designs and provide guidance for following best statistical practices and achieving appropriate statistical power [@BlairCooperCoppock:2019].

Researchers can also leverage exogenous manipulation to approximate the notion of random assignment. These approaches are particularly valuable for evaluating policies where randomized experiments would be impractical or unethical. Difference-in-differences (DiD) methods exploit policy changes that affect some units but not others, comparing outcome trends between treated and untreated groups before and after intervention. The key identifying assumption for this approach is the notion of "parallel trends"—in the absence of treatment, the difference between the treated and control groups would have remained constant over time [@Goodman-Bacon:2021]. As long as this assumption holds, researchers can construct a plausible counterfactual prediction of the outcome for treated units in the absence of treatment. @CardKrueger:1994 used DiD to measure the effect of minimum wage increases on employment by comparing New Jersey (which raised its minimum wage) to neighboring Pennsylvania (which did not). Their work has since inspired a large literature of research across diverse policy domains including healthcare reforms, environmental regulations, educational interventions, and labor market policies [@RothSantAnnaBilinski:2023]. Recent methodological advances include two-way fixed effects (TWFE) models that expand the philosophy of DiD to allow for multiple time periods and staggered treatment adoption [@CallawaySantAnna:2021], as well as techniques to address potential heterogeneous treatment effects over time [@SunAbraham:2021]. Related to DiD, newer synthetic control methods (SCM) allow researchers to simulate the counterfactual trajectory of treated units by modeling the pre-treatment characteristics of untreated units [@AbadieDiamondHainmueller:2015; @AbadieDiamondHainmueller:2010a]. Or, in other words, if one state implements a policy in a given year, the observed characteristics of other states are used to construct a synthetic version of the state, and researchers can calculate the difference between the actual and synthetic outcomes to determine the causal effect.

Regression discontinuity design (RDD) methods rely on a special circumstances where treatment assignment is determined by an arbitrary threshold in a running variable that determines program eligibility, like income or test scores. Leverage for identification comes from the assumption that units right around the threshold are essentially the same except for their treatment status. For instance, if a poverty intervention program is only available to people earning less than 100% of the federal poverty line, people who are at 99% and 101% of the poverty line likely have very similar socioeconomic backgrounds and can be compared as if they were randomly assigned to the program. Under this assumption, researchers calculate the difference in average outcomes for units within a narrow bandwidth around the threshold (e.g., comparing those at 95–99.9% of the poverty line with those at 100.1–105%). RDD has been used for a variety of policy questions, including evaluating the effects of educational interventions that use test score cutoffs [@AngristLavy:1999], age-based eligibility for government programs [@CardShore-Sheppard:2004], and geographic boundaries that determine exposure to political advertisements and their effect on voter turnout [@KeeleTitiunik:2015].

One final circumstantial approach is use an instrument—or a completely exogenous source of variation—that influences treatment assignment but influences the outcome *only through* its effect on treatment [@AngristPischke:2008]. Researchers use the variation in the instrumental variable (IV) to account for the endogeneity or unobserved confounding in the relationship between treatment and outcome, thus resulting in a plausible causal effect. This approach was popular in the 1990s and early 2000s, with research using proximity to college as an instrument for education to estimate the effect of education on lifetime earnings [@Card:1995], or using rainfall as an instrument for economic growth to estimate the effect of development on civil conflict [@MiguelSatyanathSergenti:2004]. However, satisfying the requirements of a valid instrument—that it is relevant (strongly correlated with the treatment), exclusive (correlated with the outcome only through the treatment), and exogenous (not correlated with any omitted variables)—has proven difficult. For example, @Mellon:2024 identifies nearly 200 violations of the exclusion assumption for weather-based instruments like rainfall. Convincing instruments are increasingly hard to come by. 

Circumstantial identification has seen rapid methodological innovation since the early 2000s, particularly in applied econometric research [@AngristPischke:2008; @AngristPischke:2015]. The 2019 and 2021 Nobel Memorial Prizes in Economic Sciences were awarded to researchers dedicated to circumstantial identification: Abhijit Banerjee, Esther Duflo, and Michael Kremer in 2019 for their work on RCTs; and David Card, Joshua Angrist, and Guido Imbens in 2021 for their work on quasi-experimental research designs. These awards highlight the effect these methods have had on evidence-based policy research and on our understanding of causal inference in social science more broadly.

## Prediction

While explanatory analysis explores the associations and effects of individual explanatory variables ($X$) on outcomes ($Y$) to determine why phenomena exist and how they can be influenced, predictive analysis focuses on what will happen next by either forecasting continuous outcomes and classifying categorical outcomes. Prediction is far less common than explanation in academic policy research—the majority of articles published by *JPAM* and other policy analysis journals answer explanatory questions, generally using causal inference, to explore the effects of specific policy interventions. However, forecasting and classification play crucial roles in practical governance and policy implementation.

Forecasting involves predicting numeric values. The Congressional Budget Office forecasts the economic costs of proposed federal legislation, the Federal Reserve publishes projections of inflation and GDP, states estimate future tax revenues to inform budget planning [@Dadayan:2024; @McNichol:2014]. Public health agencies forecast disease spread and healthcare utilization, while transportation departments predict traffic flow and urban congestion [@HoqueErhardtSchmitt:2021]. Social service agencies use prediction to anticipate state welfare caseloads [@Nadal-FernandezPepinSchrader:2025; @GurmuSmith:2008] and unemployment insurance claims [@ChatterjiHanLahiri:2022]. Classification, meanwhile, assigns units to discrete categories based on their observable characteristics. Credit risk assessment models classify loan applicants by default risk [@MeursaultMoultonSantucci:2024], school districts deploy early warning systems to identify students at risk of dropping out [@BirdCastlemanSong:2024], law enforcement agencies use predictive policing algorithms to allocate patrol resources based on forecasted crime patterns [@Lau:2020], and criminal justice agencies set bail based on risk categorization [@BerkHeidariJabbari:2021]. These forecasts inform resource allocation decisions and policy design and are central to daily public management.

The statistical techniques for prediction differ substantially from those used for explanation [@JamesWittenHastie:2021]. While explanatory modeling focuses on parameter estimation and hypothesis testing, predictive modeling focuses on accuracy and performance on new, unseen data. Predictive modeling typically involves partitioning data into training and testing sets. Analysts fit a model on the training data, then evaluate its performance on the reserved test data to assess generalizability. Models range in complexity. Standard regression models can be used to generate predictions of $Y$ by plugging test data into an estimated model, but more sophisticated models tend to yield better predictions. For instance, time series methods like ARIMA and exponential smoothing models account for temporal patterns in data [@HyndmanAthanasopoulos:2021], while machine learning approaches—including decision trees, random forests, support vector machines, and neural networks—can capture complex, nonlinear relationships without requiring explicit specification of functional forms [@AtheyImbens:2019]. 

Unlike explanatory models, where researchers interpret coefficients to understand the effect of individual $X$ variables, the parameters in many predictive models (particularly machine learning approaches) are often uninterpretable "black boxes" [@AtheyImbens:2019]. Instead of checking if coefficients are statistically significant or robust to different model specifications, researchers assess the performance of these models based on their predictive accuracy. For example, cross-validation procedures and metrics like the root mean squared error (RMSE) and the area under the receiver operating characteristic curve (AUC-ROC) provide common measures of model performance.

Despite their practical value, predictive techniques are underrepresented in policy analysis research. However, recent methodological innovations have begun to bridge the gap between prediction and explanation. @AtheyImbens:2019 call for increased use of machine learning in econometric and policy research and provide an overview of how these techniques can complement more explanatory work. Ongoing work in sociology and econometrics seeks to combine the predictive power of machine learning while also estimating causal effects and understanding mechanisms [@BrandZhouXie:2023; @WagerAthey:2018; @SemenovaChernozhukov:2021], essentially allowing for $X$-focused work using methods designed for predicting $Y$. 


# The pitfalls of counting, gathering, and learning from public data

This abundance of high quality data and rigorous descriptive, explanatory, and predictive methods provides policy researchers with ample evidence and tools to test theories, evaluate policies, and refine public and nonprofit programs. However, the collection and analysis of data by governments has also faced significant criticism. @FourcadeHealy:2024 describe the emergence of a three-part "data imperative," where the public and private sectors are both driven by social pressures to *count*, *gather*, and *learn* from data. As noted earlier, governments have long sought to count the social, political, economic, and demographic activities that occur under their purview. Yet, when deciding which phenomena to count and how to count them, government data collection processes can conflict with values like democratic responsiveness, leading to biased and potentially harmful results.

In the late 1800s, as both private firms and government agencies collected more records about customers and citizens, organizations sought to systematize and order this data. Insurance companies, financial firms, real estate lenders, and government benefits agencies used observable individual-level data to organize people into aggregated categories related to health, financial risk, and social status. Specific types of readily measurable individual characteristics—such as income, sex, race, ethnicity, occupation, and education—became standardized and were used both to increase private sector profits and enhance state control by making the population "legible" [@Scott:1998]. Throughout the 1900s, broad social and economic indicators like poverty measures and gross domestic product (GDP) went through a similar process of aggregation and standardization [@Karabell:2014]. The imperative to count continues today, as seen throughout this essay.

This pursuit of legibility, however, has created a false "impression of precision and order" [@FourcadeHealy:2024, 71], where data-based policies and decision-making can *feel* systematic, scientific, and objective while failing to account for individual heterogeneity. Ordering society into easily observable categories inherently privileges certain types of measurable characteristics by flattening more nuanced details about individuals into homogenous categories. @Scott:1998 argues that this oversimplification of society into quantifiable numbers led to the erasure of local knowledge and the imposition of top-down policies disconnected from local realities, often with disastrous consequences.

One commonly proposed solution for restoring local expertise to data collection has been to democratize the process for deciding what to count and allow for public participation in social scientific work [@Kitcher:2001]. Doing so arguably allows for greater diversity in the values that get embedded in social indicators. For example, inflation in the United States is measured with the Consumer Price Index, which tracks the relative prices of items in a basket of goods that reflect typical household needs. The components of the CPI are countable, legible, and measurable, but they are also laden with values pertinent to specific segments of society—the basket of goods used in the index represents the consumption habits of wealthier households [@Thoma:2024, 7]. Policy makers then use measures like the CPI as objective indicators of economic health and create corresponding policies that privilege those who are reflected in the indicator. @Thoma:2024 argues that aggregate, seemingly-objective indicators like the CPI are anti-democratic. It is possible to democratize the process of creating these indicators—for instance, people from other socioeconomic backgrounds could suggest other unmeasured CPI components that better reflect their lived reality. But even with this kind of citizen input, epistemic inequalities remain—marginal changes to components of an aggregate measure still imply that the concept being counted by the measure is important and valuable. Individuals and social groups who might not benefit from or who disagree with this data collection are left out of the process [@Thoma:2024], and data continues to be counted without them.

Accompanying the imperative to *count* social data is the imperative to *gather* as much data as possible. This injunction to collect data has been driven by multiple factors. First, it is helpful for a state to be able to see across agencies and units (e.g., government benefits offices need income information held by tax agencies), and gathering details about individuals can improve citizen experiences with government services—linking databases across agencies was a key goal of improving digital governance in the 2010s [@Noveck:2015]. Beyond the intentional work of collecting data, some of the rise in data collection has been a consequence of sociological isomorphism and the reduced cost of gathering and storing data. Institutions observe how more prominent organizations collect detailed data and do the same, following a mimetic logic of collecting more data "because that's what leaders in our field do—and so they must be good" [@FourcadeHealy:2024, 78]. Moreover, the cost of collecting and storing exceptionally detailed data has decreased substantially over the past decades, and data collection platforms have made it easy for both organizations and individual policy researchers to pick up incidental data about people. For instance, by default, survey platforms include a surprising amount of metadata about respondents including IP addresses, geolocation data, and time spent on the survey. With additional website analytics, it is possible to collect even more identifiable data, such as hardware addresses, the URLs of websites that referred users to the survey, physical home addresses, and other details. Researchers and analysts tend to collect and retain this data "just in case," with the hope that it may someday prove to be useful. Intelligence agencies like the National Security Agency have partnered with private data brokers since 2007 to collect massive amounts of information on US residents [@Savage:2024], most of which reside unused in data warehouses. While these massive repositories of data—often collected mimetically and with no explicit purpose—can be stored relatively cheaply, securing them against data breaches and cyberattacks and making them safely accessible to the public poses substantial liability and costs. For instance, in 2020, a large-scale attack in the United States targeted personnel data housed by the Office of Personnel Management, the Pentagon, and the State, Treasury, and Justice Departments [@SangerPerlrothSchmitt:2020]. Data breaches at state and local government agencies have proven even more common and consequential, exposing the personal records of millions of residents, including the leak of nearly 20 million voter records in California in 2017 and 16 million payment records in Indiana in 2018 [@Bischoff:2019]. Threats to citizen privacy have even been caused by governments themselves, with federal agencies allowing private data-mining firms like Palantir to trawl through warehouses of unused data with little oversight or accountability [@Bennett:2025]. These threats to government-gathered data underscore the risks of unchecked data accumulation and point to a need for more deliberate and purpose-driven public sector data collection practices.

The final mandate after the injunctions to count and gather is the imperative to *learn* from the collected data. There are many good, scientifically sound, and equitable methods for describing, explaining, and predicting social phenomena related to the public sector. At the same time, though, "learning from data at any scale and scope is easy to do badly" [@FourcadeHealy:2024, 88]. As discussed earlier, data counted and gathered by governments does not always reflect population characteristics and inherently encodes epistemic inequalities that favor social majorities [@Thoma:2024]. A rich literature demonstrates that analyses based on this underlying data are also systematically biased against women, racial and ethnic minorities, and disabled communities [@Criado-Perez:2020; @DIgnazioKlein:2020; @Broussard:2023]. These biases often stem from historical underrepresentation in data collection, the use of flawed proxies for complex social phenomena, and the epistemic prioritization of majority perspectives in defining what counts as valid data.

More concerning for policymakers is the fact that policy decisions are often made based on analyses of this biased data without recognition of that bias. Instead, researchers and practitioners often assume that since quantitative policy research is based on hard numbers, it is inherently objective and bias-free [@ONeil:2016]. This is especially common with more advanced black-box predictive modeling systems, which typically lack regulation and scrutiny over the resulting predictions. For instance, during the first few months of the COVID-19 pandemic, algorithms used by state unemployment benefits offices incorrectly categorized thousands of applicants as ineligible for unemployment insurance support because of minor inconsistencies in their records [@Pahlka:2023]. Misclassification can have more serious legal consequences too. @Eubanks:2019 describes how many state-run child protective services agencies have turned to machine learning-based systems for predicting child abuse, where automated predictions for possible domestic abuses in households trigger automated actions by state agencies and impose automated surveillance and behavioral requirements with strict legal consequences for noncompliance. This process has few humans in the loop to oversee possible errors, and families suffer from false positives flagged by the system.

Other biased automated policy outcomes abound. In 2024, several US cities cancelled their contacts with the ShotSpotter gunfire detection and classification algorithm following research that demonstrated that it was both racially biased and ineffective at increasing arrests, reducing crime, or detecting gun violence [@DoucetteGreenNecciDineen:2021]. Automated systems misidentify Black criminal suspects [@Broussard:2023; @AngwinLarsonMattu:2016], set higher bail amounts for Black defendants [@AngwinLarsonMattu:2016; @KoepkeRobinson:2018], automatically flag trans people at security checkpoints [@Costanza-Chock:2018], prefer white male graduate school applicants [@Burke:2020], offer better loans to white male lenders [@Miller:2020], and are less likely to recommend hiring non-white, male job applicants [@JaserPetrakakiStarr:2022]. Following @Thoma:2024, these methods reflect the epistemic inequality inherent in the underlying data.

Dealing with the bias and incompleteness of these predictive models is made more difficult due to the complexity and opaqueness of their underlying statistical methods. At their core, predictive methods use specialized statistics to recognize patterns and make guesses about future events based on those patterns. These algorithms and models construct their own "sense" of the world—similar to how governments seek to make society "legible" [@Scott:1998]—but lack human context for why specific patterns exist in the first place. Human, street-level bureaucrats can recognize why race, gender, disability, and other personal characteristics might influence someone's interaction with the government and can make personalized accommodations as needed [@AlkhatibBernstein:2019], but algorithms cannot. Predictive models create a sort of "average" flattened utopia based on incomplete training data where the world is legible to algorithms. As a result, people who do not fit the model's sense of the world are flagged as anomalies, judged, and punished [@Alkhatib:2021]. Compounding the issue, the creators of these systems purposely market their products with overly-ambitious outcomes—criticized by some as modern "snake oil" [@NarayananKapoor:2024]—many public agencies are lured into using these products, leading to worse outcomes for the public.

Statistical methods offer useful tools for understanding social phenomena and evaluating interventions, but these same tools can reinforce existing inequalities when applied uncritically. The imperatives to count, gather, and learn from data have pose challenges for democratic accountability. Policy researchers must rely on ethical frameworks that center human values, recognize the politics of measurement, and remain attentive to voices traditionally marginalized in data collection and analysis [@OECD:2021].


# Future directions

Statistical work has long been a key component of public policy research, and will continue to play an important role in governance in the future. In conjunction with the modern emphasis on program and policy evaluation, the public sector turned toward digital governance in the 2010s. In 2011, Prime Minister David Cameron established the United Kingdom's Government Digital Service (GDS) unit, charged with "setting, leading and delivering the vision for a modern digital government" [@GovernmentDigitalService:2025]. Many other countries copied this approach by creating similar units and offices, including the United States Digital Service (USDS), established by President Barack Obama in 2014. These special units' missions are designed primarily to modernize outdated government systems and improve constituent experiences with government services—e.g. updating COBOL codebases first written in the 1960s, conducting user experience research on how applicants to benefits programs move through the system, and rapidly fixing the HealthCare.gov website that accompanied the Affordable Care Act [@Pahlka:2023]. As a part of their missions to digitize public sector services, these units also encouraged more modern forms of data analysis and statistical work. In 2015, President Obama appointed the first United States Chief Data Scientist in the Office of Science and Technology Policy, with the mission to "responsibly unleash the power of data to benefit all Americans" [@Honey:2016]. Under this mandate, USDS and OSTP encouraged data sharing across federal, state, and local agencies and supported modern and open analysis pipelines of this data, using both standard statistical techniques and more advanced machine learning approaches to help policymakers make data-driven decisions.

Since the mid-2010s, advocacy groups, activists, think tanks, and public policy schools have embraced and encouraged this governmental turn toward open data practices. Much of this work targets ethics: @Tauberer:2014 outlines 14 principles for open government data, including accessibility practices, the use of open formats, commitment to public input, and the importance of citizen privacy, while the Urban Institute [@UrbanInstitute:2025], the IAPP [@IAPP:2025], and other associations publish reports and guidelines for best data and analytics practices and lobby for digital governance policies. Policy schools have created undergraduate and graduate courses, certificates, and degree programs in policy analytics, where the techniques and methods from the field of data science are applied to issues specific to the public and nonprofit sectors. The Data Science for Public Service Consortium^[See <https://ds4ps.org/consortium/>] comprises a community of dozens of public affairs programs with policy analytics curricula and provides a space for sharing teaching materials on key statistical and quantitative methods, including a set of modern data science competencies [@DataScienceforPublicServiceConsortium:2022] like data visualization, causal inference, predictive modeling, data mining, spatial analytics, and other skills like report automation, programming, data cleaning, and project management. 

This emphasis on data science—rather than basic statistics—in public policy education parallels broader movements toward open research practices in academia and transparency in policy analytics. Academic journals increasingly require that authors share their data and code as a condition for publication, and funding agencies mandate data management plans that allow for broad accessibility and reuse. Researchers can post preregistered hypotheses, pre-analysis plans, code, and data at the Center for Open Science's Open Science Framework (OSF), the Experiments in Governance and Politics (EGAP) initiative, and the American Economic Association's Randomized Controlled Trial Registry [@TheAbdulLatifJameelPovertyActionLab:2025]. These practices not only enhance the rigor of policy research [@AndrewsKasy:2019; @BanerjeeDufloFinkelstein:2020; @FieldWagenmakersKiers:2020] but also democratize access to evidence, allowing a wider range of stakeholders to engage with, evaluate, reuse, replicate, and expand policy data and analysis.

Statistical work in public policy is not simply a technical exercise—it is fundamentally value-laden and political. As seen throughout this essay, as governments work to "see like a state" the imperatives to count, gather, and learn from data can conflict with democratic values. The choice of what to measure, how to classify observations, which analytical techniques to apply, and how to interpret results all reflect implicit value judgments that shape policy outcomes. Concerns about data and algorithmic bias in criminal justice, healthcare, education, and social services highlight how seemingly objective statistical methods can reproduce and amplify societal inequalities when deployed without critical reflection. Future work in policy-oriented data analysis must merge technical rigor with ethical awareness. The USDS motto to "responsibly unleash the power of data to benefit all" captures this aspiration well. Statistical work should be driven by and accountable to public service values. As statistical methods continue to evolve and public data grows ever more abundant, maintaining this public service orientation is essential to ensure that policy research contributes to more effective, equitable, and democratic governance.


# References {.unnumbered}
